{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxQ0Sa2+5KAJXJCPJjPVZb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewebeatty/colabassignment/blob/main/Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Neural Network Model to Produce Predictions of Success in Residential Treatment**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tt0kAsDJ0fCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this Colab notebook, where our objective is to build a neural network model capable of predicting whether an adolescent will successfully complete the program based on their demographic characteristics and initial score on the Youth Outcome Questionnaire (YOQ). The YOQ is a comprehensive 64-item self-report measure that assesses various aspects of psychological health.\n",
        "\n",
        "The successful development of this model can serve two valuable purposes. Firstly, it could facilitate predictions prior to clients' admission into the program, aiding in informed admissions decisions. Secondly, it may assist therapists and staff in adopting a more proactive approach to treatment.\n",
        "\n",
        "For our analysis using the R programming language, we will be utilizing publicly available and de-identified data from a long-term residential treatment center catering to adolescent girls with borderline tendencies. This dataset comprises demographic information, treatment-related variables, responses to all 64 YOQ questions, calculated scores for the six YOQ subscales, and an overall YOQ total score. Additionally, the dataset includes an outcome variable, denoting whether the adolescent successfully completed the program, in contrast to dropping out or being asked to leave.\n",
        "\n",
        "In regards to this assignment and code, this framework and work flow was learned from John Curtin's machine learning class. The workflow is taken from our unit on neural networks, however the code has been changed to accomodate data from my own research!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BCeT-4n564Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Required Packages**\n",
        "\n",
        "The following packages will be needed for building this model. Some packages will not be used outright, but some functions will have a dependency on these packages. When using R, we need to use the Keras package in addition to Tensorflow. We will be using tidymodels and a tidymodels style to set up and run our model."
      ],
      "metadata": {
        "id": "KJz6-Y5XACf7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTZDpuIghsym"
      },
      "outputs": [],
      "source": [
        "install.packages('tidymodels') # for modeling\n",
        "install.packages(\"psych\") # for viewing data and summary stats\n",
        "install.packages('tidyverse') # for general data wrangling\n",
        "install.packages('kableExtra') # for displaying formatted tables w/ kbl()\n",
        "install.packages('skimr') # for skim()\n",
        "install.packages('corrplot')\n",
        "install.packages('janitor')\n",
        "install.packages('cowplot') # for plot_grid() and theme_half_open()\n",
        "install.packages('ggplot2') # for plotting performance\n",
        "install.packages(\"keras\") # for NN - needed layer for R\n",
        "install.packages(\"tensorflow\") # for NN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the Required Packages**\n",
        "\n",
        "Now that we have all the packages installed, let's load them into our environment. We will also be using some functions that John Curtin wrote and has posted in to his github. We will be pulling down these functions from github using devtools. If this method doesn't work for you the functions can also be found in the file \"fun_modeling.R\" which is included in the repo."
      ],
      "metadata": {
        "id": "a5zhTl3_A4mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the required libraries, set plotting theme, and source functions through git and file\n",
        "library(ggplot2)\n",
        "#theme_set(theme_half_open()) # plotting theme\n",
        "#source('fun_modeling.R')\n",
        "library(keras)\n",
        "library(tensorflow)\n",
        "library(psych) # for summary of data\n",
        "library(tidymodels) # for modeling\n",
        "library(tidyverse) # for general data wrangling\n",
        "library(kableExtra) # for displaying formatted tables w/ kbl()\n",
        "library(skimr) # for skim()\n",
        "library(corrplot)\n",
        "library(ggplot2)\n",
        "library(cowplot)\n",
        "devtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true\") # functions for plotting\n",
        "devtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true\") # other functions that might come in handy"
      ],
      "metadata": {
        "id": "5TEfp-kFif5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load in the Data**\n",
        "The data file included in the repo is called \"yoq_nn.csv\". You will need to upload this folder to your working files in Colab to read it in."
      ],
      "metadata": {
        "id": "y0hoCdUovyyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d <- read.csv('yoq_nn.csv') # read the file in\n",
        "describe(d) # get a quick look at our data set"
      ],
      "metadata": {
        "id": "QIswTb9ajqc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data Into Training and Testing Sets\n",
        "\n",
        "We first need to split our full data into a training set for our model to learn from, and then a test set to evaluate our models performance in cases that it has not seen before."
      ],
      "metadata": {
        "id": "zOPhygxzHtLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits <- d %>%\n",
        "  initial_split(prop = 0.75, strata = \"Completion\") # splitting our data, stratifying the 0.75 (3/4) split on our outcome variable\n",
        "\n",
        "data_trn <- analysis(splits) # saving it into our training set\n",
        "data_trn %>%  nrow() # get count of training set rows, see if split seems right\n",
        "\n",
        "data_test <- assessment(splits) # saving it into our test set\n",
        "data_test %>% nrow() # get count of test set rows, see if split seems right"
      ],
      "metadata": {
        "id": "rZ4tAFYkijRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Random Seed\n",
        "For reproducibility we will set a random seed. There are points in the fitting process of our neural network model where random numbers are needed by Keras, including when we initialize the weights for the hidden and output layers, selecting units for dropout, and when selecting batches within the epochs. Thankfully, tidymodels lets us provide three seeds to these points within the model reproducible."
      ],
      "metadata": {
        "id": "pWNGuV7zFxHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set.seed(12345) # random seed\n",
        "fit_seeds <- sample.int(10^5, size = 3) # we will be using a random seed within our model, so we are saving it here"
      ],
      "metadata": {
        "id": "mMopqSgMFwwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up K-Fold Splits\n",
        "We will be using k-fold cross-validation for our neural network. This is advantageous because we are dealing with a fairly small dataset. By using k-fold cross-validation we have better data utilization by dividing the dataset into k subsets (\"or folds\"). Each fold serves as a validation set in every iteration, while the remaining k-1 folds are used for training. This maximizes the use of our data, enhances the model's overall robustness, and reduces overfitting.\n",
        "\n",
        "K-fold cross-validation also provides more reliable performance estimation by averaging performance metrics over *k* iterations. This helps assess the model's consistency and generalization on unseen data, which could vary greatly with a small data set like ours. Additionally, the k-fold cross-validation allows for hyperparameter tuning, as it allows evaluating different parameter configurations across various k subsets. This ultimately leads to more informed hyperparameter selection, ensuring a more stable performing neural network model, even with our small data set!"
      ],
      "metadata": {
        "id": "-h1sDb4JRp5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits_kfold <- data_trn %>%\n",
        "  vfold_cv(v = 10, repeats = 1, strata = \"Completion\") # specifying that we want 10 folds stratified on our outcome variable \"Completion\" with just one repeat"
      ],
      "metadata": {
        "id": "pp2Z3VpWiHl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up a Recipe\n",
        "\n",
        "In the tidymodels framework, a \"recipe\" serves as a data preprocessing/feature engineering for transforming our raw data into a format that is suited for training and evaluating our neural network model. Normally, recipes facilitate steps like scaling, normalization, handling missing values, and handling categorical variables to produce consistent and sensical input for the model.\n",
        "\n",
        "Some models require extensive data pre-processing and feature engineering to optimize results, however the nueral network does pretty well \"out of the box\", and thus minimal processing and feature engineering is required.\n"
      ],
      "metadata": {
        "id": "wU44nOcjXqZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rec <-\n",
        "  recipe(Completion ~ ., data = data_trn) %>% # regressing all variables in the data onto our outcome\n",
        "  step_string2factor(Completion, levels = c(\"completion\", \"non_completion\")) %>% # specifying the levels of our outcome variable and turning into a factor variable instead of string\n",
        "  step_YeoJohnson(all_numeric_predictors()) %>% # for normality, there is some extreme skew in some variabless\n",
        "  step_nzv(all_predictors()) %>% # removes variables that are very sparse and unbalanced (Near-Zero Variance), there are some variables like this in the dataset\n",
        "  step_impute_knn(all_numeric_predictors()) %>% # since there is missing data, we will use the knn method to impute the data\n",
        "  step_range(all_predictors()) # range correction for better model performance and convergence"
      ],
      "metadata": {
        "id": "QGQfApo4iegM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a Feature Matrix\n",
        "Now that we have a recipe, we will feed in our training data to make a feature matrix to use in our best model. We will also make a feature matrix right now for our test data, so we can see the accuracy of our model and visualise performance."
      ],
      "metadata": {
        "id": "KycV9X_vguUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feat_trn <- rec %>%\n",
        "  make_features(data_trn)\n",
        "\n",
        "feat_test <- rec %>%\n",
        "  make_features(data_trn, data_test)"
      ],
      "metadata": {
        "id": "SaINfTllgoeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper Parameter Tuning\n",
        "With the tidymodels framework we can tune our hyper parameters to find the optimal point. For our model, we are going to tune the amoung of hidden layers, and the amount of dropout."
      ],
      "metadata": {
        "id": "rEmCrTopGc3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_keras <- expand_grid(hidden_units = c(5, 10, 20), dropout = c(.1, .001))"
      ],
      "metadata": {
        "id": "cHRVMNeLGdfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit the Model\n",
        "\n",
        "We can now run our model! We will be using our grid to tune our hyperparameters."
      ],
      "metadata": {
        "id": "abucve8iNphW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fits_nn <-\n",
        "  mlp(hidden_units = tune(), dropout = tune(), activation = \"relu\", epochs = 50) %>%\n",
        "  set_mode(\"classification\") %>% # setting the mode of the model to \"classification\" since we are solving a classification problem\n",
        "  set_engine(\"keras\", verbose = 1, seeds = fit_seeds) %>%\n",
        "  tune_grid(preprocessor = rec,\n",
        "                grid = grid_keras,\n",
        "                resamples = splits_kfold,\n",
        "                metrics = metric_set(accuracy)) # 'metrics' sets the evaluation metrics, in this case, we are using accuracy as the performance metric, but can use others later"
      ],
      "metadata": {
        "id": "o5_1-6jxGfH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance of Best Model in Train\n",
        "We can use show_best to see how our best model across the parameters performed."
      ],
      "metadata": {
        "id": "8tsVCcCTOfZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_best(fits_nn)"
      ],
      "metadata": {
        "id": "T1W_Y9ReOvvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit the Best Model\n",
        "We can now fit our best model in training."
      ],
      "metadata": {
        "id": "YHZSMTQJOyG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit_nn <- mlp(hidden_units = select_best(fits_nn)$hidden_units,dropout= select_best(fits_nn)$dropout, activation = \"relu\", epochs = 50) %>%\n",
        "    set_mode(\"classification\") %>%\n",
        "    set_engine(\"keras\", verbose = 1, seeds = fit_seeds) %>%  fit(Completion ~ ., data = feat_trn)"
      ],
      "metadata": {
        "id": "h9TeAMGqPB8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy in Test Data\n",
        "We can now see how our model performs in the test data."
      ],
      "metadata": {
        "id": "RDzg09VTPD5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_vec(truth = feat_test$Completion,\n",
        "             estimate = predict(fit_nn, feat_test)$.pred_class) # grabs the true values and compares them to the predictions of the model"
      ],
      "metadata": {
        "id": "DCO-0NmXPRX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "A confusion matrix can be used to understand how the model is performing, and where it may be lacking. We can put this confusion matrix into a heatmap style plot to get a quicker understanding of how the model is performing, For our heatmap we can see that the model is doing good(ish) at predicting true completion."
      ],
      "metadata": {
        "id": "mgJVDY5HPgKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm <- tibble(truth = feat_test$Completion,\n",
        "                 estimate = predict(fit_nn, feat_test)$.pred_class) %>%\n",
        "  conf_mat(truth, estimate)\n",
        "\n",
        "cm\n",
        "\n",
        "cm <- tibble(truth = feat_test$Completion,\n",
        "                 estimate = predict(fit_nn, feat_test)$.pred_class) %>%\n",
        "  conf_mat(truth, estimate)\n",
        "\n",
        "autoplot(cm, type = 'heatmap')\n"
      ],
      "metadata": {
        "id": "EDUBN_VJQMbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other Estimates of Performance\n",
        "Earlier I mentioned that we can get estimates of our model other than accuracy. This is important because sometimes accuracy is not a good estimate of our models performance if we have a specific goal in mind of how we want to use the model. For example, our model has okay accuracy and sensitivity, but falls apart when it comes to specificity, which ultimately is not incredible helpful when we want to use this model to intervene in cases where we think non-completion might happen."
      ],
      "metadata": {
        "id": "0QtfpvJ0QP2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm %>%\n",
        "  summary() %>%\n",
        "  filter(.metric %in% c(\"accuracy\", \"sens\", \"spec\")) %>%\n",
        "  dplyr::select(-.estimator)"
      ],
      "metadata": {
        "id": "oHzPRP5JQ4Mu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}